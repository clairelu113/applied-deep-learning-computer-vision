# Biweekly Report

# Claire Lu

Batch Normalization stabilizes the internal feature distributions, reducing shifts in mean and variance across training epochs. This stabilization leads to smoother optimization and faster convergence. To demonstrate, I'll compare training dynamics, gradiants, and internal covariate shift between models with and without batch normalization.

Citation Request:
https://arxiv.org/pdf/1502.03167